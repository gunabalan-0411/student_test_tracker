{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM (Long Short Term Memory) - Architecture Explained (Tanglish)\n",
    "\n",
    "Pazhaya notebook la namma RNN paathom. Adhula irunda 'Sequence Memory' concept super ah irundhalum, 'Vanishing Gradient Problem' nala adhu long sequence ku work aagadhu.\n",
    "\n",
    "**Example Problem**: Oru periya paragraph padichitu, kadasiya iruka kelvi ku badhil solla sonna, RNN start la padichadha marandhudum.\n",
    "Idha solve panna dhaan **LSTM** vandhuchu. Idhu 'Important' vishayatha mattum nyabagam vechikum, unwanted ah 'Forget' pannidum.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. LSTM Architecture: The Gates Concept\n",
    "\n",
    "LSTM vitruka secret weapon: **Gates**.\n",
    "Idha oru 'Water Tank' maari imagine pannikonga. Valve (Gate) open panna thanni pogum, close panna pogadhu. Adhe concept dhaan inga information flow ku.\n",
    "\n",
    "### The 3 Main Gates:\n",
    "1.  **Forget Gate ($f_t$)**: Pazhaya information la edhu theva illaiyo adha trash panna use aagum. (Example: 'He' vandha udane pazhaya 'She' context ah azhikalam).\n",
    "2.  **Input Gate ($i_t$)**: Puthiya information la edhu mukkiyam nu mudivu pannum.\n",
    "3.  **Output Gate ($o_t$)**: Current Context vechu edha veliya sollanum nu mudivu pannum.\n",
    "\n",
    "### Diagram Flow\n",
    "```mermaid\n",
    "graph LR\n",
    "    Input[Xt] --> ForgetGate\n",
    "    PrevHidden[Ht-1] --> ForgetGate\n",
    "    ForgetGate --> CellState[Ct]\n",
    "    Input --> InputGate\n",
    "    PrevHidden --> InputGate\n",
    "    InputGate --> CellState\n",
    "    CellState --> OutputGate\n",
    "    OutputGate --> HiddenState[Ht]\n",
    "```\n",
    "\n",
    "### Math Behind the Gates\n",
    "Equation paathu bayapada vendam. Concept purinja podhum.\n",
    "\n",
    "1.  **Forget**: $f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)$\n",
    "2.  **Input**: $i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)$\n",
    "3.  **Update**: $\\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)$\n",
    "4.  **Cell State**: $C_t = f_t * C_{t-1} + i_t * \\tilde{C}_t$\n",
    "5.  **Output**: $o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)$\n",
    "6.  **Hidden State**: $h_t = o_t * \\tanh(C_t)$\n",
    "\n",
    "*Note*: $\\sigma$ (Sigmoid) is 0 or 1. (0 = Close Gate, 1 = Open Gate).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PyTorch LSTM Implementation\n",
    "\n",
    "`nn.LSTM` use panni model create pannuvom. Difference between RNN and LSTM code is simple - Input arguments same dhaan, output la mattum `(HiddenState, CellState)` rendume varum (RNN la HiddenState mattum varum).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_size = 5 # E.g., Normalized Word Vector size\n",
    "hidden_size = 10 # Memory size\n",
    "num_layers = 2 # Deep LSTM (Stacked)\n",
    "batch_size = 3 \n",
    "seq_len = 4 # 'I love Machine Learning' (4 words)\n",
    "\n",
    "# Create LSTM Model\n",
    "# Batch First = True na Input shape (Batch, Seq, Features) irukanum\n",
    "lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "# Inputs\n",
    "inputs = torch.randn(batch_size, seq_len, input_size)\n",
    "\n",
    "# Initial States (Hidden State h0, Cell State c0)\n",
    "h0 = torch.zeros(num_layers, batch_size, hidden_size)\n",
    "c0 = torch.zeros(num_layers, batch_size, hidden_size)\n",
    "\n",
    "# Forward Pass\n",
    "# Output shape: (Batch, Seq, HiddenSize)\n",
    "# Final States shape: (Num_layers, Batch, HiddenSize)\n",
    "out, (hn, cn) = lstm(inputs, (h0, c0))\n",
    "\n",
    "print(\"Input Data Shape:\", inputs.shape)\n",
    "print(\"Output Data Shape:\", out.shape)\n",
    "print(\"Final Hidden State Shape:\", hn.shape)\n",
    "print(\"Final Cell State Shape:\", cn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Practical Use Case: Sentiment Analysis (Logic)\n",
    "\n",
    "LSTM vechu 'Positive' or 'Negative' nu kandupudika use aagum.\n",
    "\n",
    "**Flow:**\n",
    "1. Text -> Embedding (Numbers)\n",
    "2. Embedding -> LSTM Layer\n",
    "3. LSTM Last Output -> Fully Connected Layer (Classifier)\n",
    "4. Output -> Sigmoid (Probability 0-1)\n",
    "\n",
    "Indha architecture dhaan 'Siri', 'Google Translate' start aanapo base ah irundhuchu. But ippo **Transformers** dhaan ruling. Adha next notebook la paapom!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“š LSTM Complete Summary \n",
    "\n",
    "**Instructions:** Copy this entire content and paste it as new cells in your LSTM_Architecture_Tanglish.ipynb notebook\n",
    "\n",
    "\n",
    "\n",
    "## 1ï¸âƒ£ LSTM Enna? Yean Venum? (What is LSTM? Why Do We Need It?)\n",
    "\n",
    "### RNN oda Problems:\n",
    "\n",
    "âŒ **Vanishing Gradient Problem** - Long sequences la pazhaya info ah remember pana mudiyathu  \n",
    "âŒ **No Control** - Edha remember pananum, edha forget pananum nu control illa  \n",
    "âŒ **Short-term Memory** - Arambathula padichadha marandhudum\n",
    "\n",
    "### LSTM - The Solution! ðŸ’¡\n",
    "\n",
    "- **LSTM = Long Short-Term Memory**\n",
    "- RNN oda advanced version\n",
    "- **Gates** mechanism use panni, edha remember pananum, edha forget pananum nu decide pannum\n",
    "- Long-term dependencies ah handle pana best!\n",
    "\n",
    "**Real-Life Example:**\n",
    "\n",
    "```\n",
    "\"I grew up in France... (100 words later)... I speak fluent ____\"\n",
    "```\n",
    "\n",
    "- RNN: \"France\" ah marandhidum â†’ Thappu answer\n",
    "- LSTM: \"France\" ah remember vechukum â†’ \"French\" nu correct ah predict pannum\n",
    "\n",
    "---\n",
    "\n",
    "## 2ï¸âƒ£ LSTM Architecture - The 3 Gates System ðŸšª\n",
    "\n",
    "LSTM la **4 main components** irukum:\n",
    "\n",
    "1. **Cell State (C_t)** - Long-term memory storage (Main highway)\n",
    "2. **Hidden State (h_t)** - Short-term memory (Current output)\n",
    "3. **Three Gates** - Information flow control pannum\n",
    "\n",
    "### Gate 1: Forget Gate (f_t) ðŸ—‘ï¸\n",
    "\n",
    "**Purpose:** Pazhaya information la edha delete pananum nu decide pannum\n",
    "\n",
    "**Formula:**\n",
    "$$f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)$$\n",
    "\n",
    "**Example:**\n",
    "\n",
    "- Input: \"The cat was black. **The dog** was white.\"\n",
    "- Forget Gate: \"cat\" pathi info ah forget pannidum (because new subject \"dog\" vandhuduchu)\n",
    "\n",
    "**How it works:**\n",
    "\n",
    "- Sigmoid function use pannum (output: 0 to 1)\n",
    "- 0 = Completely forget\n",
    "- 1 = Completely remember\n",
    "\n",
    "---\n",
    "\n",
    "### Gate 2: Input Gate (i_t) ðŸ“¥\n",
    "\n",
    "**Purpose:** Puthiya information la edha add pananum nu decide pannum\n",
    "\n",
    "**Formulas:**\n",
    "$$i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)$$\n",
    "$$\\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)$$\n",
    "\n",
    "**Example:**\n",
    "\n",
    "- New word: \"beautiful\"\n",
    "- Input Gate: Idhukku important nu mudivu panni cell state ku add pannum\n",
    "\n",
    "**Two Steps:**\n",
    "\n",
    "1. **Decide** what to add (sigmoid function)\n",
    "2. **Create** candidate values (tanh function: -1 to +1)\n",
    "\n",
    "---\n",
    "\n",
    "### Gate 3: Output Gate (o_t) ðŸ“¤\n",
    "\n",
    "**Purpose:** Current step la enna output kudukanum nu decide pannum\n",
    "\n",
    "**Formulas:**\n",
    "$$o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)$$\n",
    "$$h_t = o_t \\cdot \\tanh(C_t)$$\n",
    "\n",
    "**Example:**\n",
    "\n",
    "- Sentence half complete: \"The weather is\"\n",
    "- Output Gate: Current context based ah \"sunny\" or \"rainy\" nu predict pannum\n",
    "\n",
    "---\n",
    "\n",
    "### Cell State Update ðŸ”„\n",
    "\n",
    "**Formula:**\n",
    "$$C_t = f_t \\cdot C_{t-1} + i_t \\cdot \\tilde{C}_t$$\n",
    "\n",
    "**Breakdown:**\n",
    "\n",
    "- `f_t Â· C_{t-1}` â†’ Pazhaya memory la edha keep pananum\n",
    "- `i_t Â· CÌƒ_t` â†’ Puthiya info la edha add pananum\n",
    "- Both ah combine panni new cell state create pannum\n",
    "\n",
    "---\n",
    "\n",
    "## 3ï¸âƒ£ LSTM vs RNN - Quick Comparison\n",
    "\n",
    "| **Feature**            | **RNN**         | **LSTM**                        |\n",
    "| ---------------------- | --------------- | ------------------------------- |\n",
    "| **Memory**             | Short-term only | Long-term + Short-term          |\n",
    "| **Gates**              | No gates        | 3 Gates (Forget, Input, Output) |\n",
    "| **Vanishing Gradient** | Problem irukum  | Almost solved                   |\n",
    "| **Complexity**         | Simple          | Complex (more parameters)       |\n",
    "| **Training Time**      | Fast            | Slower                          |\n",
    "| **Use Cases**          | Short sequences | Long sequences                  |\n",
    "\n",
    "\n",
    "\n",
    "## 4ï¸âƒ£ LSTM Variants (Different Types)\n",
    "\n",
    "### a) Vanilla LSTM (Standard LSTM)\n",
    "\n",
    "- 3 gates + cell state\n",
    "- Most commonly used version\n",
    "\n",
    "### b) Peephole LSTM\n",
    "\n",
    "- Gates ku direct ah cell state access kudukurom\n",
    "- More control over information flow\n",
    "\n",
    "### c) Deep LSTM (Stacked LSTM)\n",
    "\n",
    "- Multiple LSTM layers one after another\n",
    "- Complex patterns learn pana use aagum\n",
    "\n",
    "### d) Bidirectional LSTM\n",
    "\n",
    "- Forward + Backward directions la process pannum\n",
    "- Full context (past + future) therijrum\n",
    "- **Use case:** Named Entity Recognition, Speech Recognition\n",
    "\n",
    "### e) GRU (Gated Recurrent Unit)\n",
    "\n",
    "- Simplified LSTM version\n",
    "- Only 2 gates (Update gate + Reset gate)\n",
    "- Faster training, LSTM ku similar performance\n",
    "\n",
    "\n",
    "\n",
    "## 5ï¸âƒ£ GRU vs LSTM - Detailed Comparison\n",
    "\n",
    "| **Aspect**      | **LSTM**                             | **GRU**                                   |\n",
    "| --------------- | ------------------------------------ | ----------------------------------------- |\n",
    "| **Gates**       | 3 Gates                              | 2 Gates                                   |\n",
    "| **Cell State**  | Separate cell state irukum           | No separate cell state                    |\n",
    "| **Parameters**  | More parameters                      | Less parameters                           |\n",
    "| **Training**    | Slower                               | Faster                                    |\n",
    "| **Memory**      | More memory needed                   | Less memory                               |\n",
    "| **Performance** | Slightly better for complex tasks    | Similar, sometimes better                 |\n",
    "| **Best For**    | Long sequences with complex patterns | Simpler sequences, faster training needed |\n",
    "\n",
    "**When to use what?**\n",
    "\n",
    "- **LSTM:** Long sequences, complex patterns, more data available\n",
    "- **GRU:** Faster training needed, less data, simpler patterns\n",
    "\n",
    "---\n",
    "\n",
    "## 6ï¸âƒ£ LSTM Applications - Real World Examples\n",
    "\n",
    "### ðŸ“ Natural Language Processing (NLP)\n",
    "\n",
    "- **Language Translation:** English â†’ Tamil conversion\n",
    "- **Text Generation:** Next word/sentence prediction\n",
    "- **Sentiment Analysis:** Movie reviews ah positive/negative nu classify\n",
    "- **Chatbots:** Conversational AI\n",
    "\n",
    "### ðŸŽ¤ Speech Recognition\n",
    "\n",
    "- **Voice Assistants:** Siri, Alexa, Google Assistant\n",
    "- **Speech-to-Text:** Audio ah text ah convert pannum\n",
    "- **Voice Commands:** \"Hey Google, turn on lights\"\n",
    "\n",
    "### ðŸ–¼ï¸ Image Processing\n",
    "\n",
    "- **Image Captioning:** Image paathu description generate pannum\n",
    "  - Example: \"A dog playing in the park\"\n",
    "- **Video Analysis:** Frame-by-frame sequence analysis\n",
    "\n",
    "### ðŸŽµ Music Generation\n",
    "\n",
    "- Notes sequence based ah new music create pannum\n",
    "- Classical music patterns learn panni generate pannum\n",
    "\n",
    "### ðŸ“ˆ Time Series Prediction\n",
    "\n",
    "- **Stock Market:** Price predictions\n",
    "- **Weather Forecast:** Temperature, rainfall predictions\n",
    "- **Sales Forecasting:** Future sales predict pannum\n",
    "\n",
    "### âœï¸ Handwriting Recognition\n",
    "\n",
    "- Handwritten text ah digital text ah convert pannum\n",
    "- Signature verification\n",
    "\n",
    "\n",
    "\n",
    "## 7ï¸âƒ£ LSTM Advantages âœ…\n",
    "\n",
    "ðŸŽ¯ **Long-term Memory** - Vanishing gradient problem ah solve pannum  \n",
    "ðŸŽ¯ **Selective Memory** - Gates use panni important info mattum remember pannum  \n",
    "ðŸŽ¯ **Flexible** - Variable length sequences handle panlam  \n",
    "ðŸŽ¯ **Versatile** - Text, audio, video, time-series data ellathukum use panlam  \n",
    "ðŸŽ¯ **Better Context** - Past information ah effectively use pannum\n",
    "\n",
    "\n",
    "\n",
    "## 8ï¸âƒ£ LSTM Limitations âŒ\n",
    "\n",
    "âŒ **Training Time** - RNN vida slow ah train aagum  \n",
    "âŒ **Computational Cost** - More parameters â†’ More memory + processing power venum  \n",
    "âŒ **Still Gradient Issues** - Completely vanishing gradient solve aagala  \n",
    "âŒ **Overfitting** - Small datasets la overfit aaga chances irukum  \n",
    "âŒ **Sequential Processing** - Parallel processing pana mudiyathu (Transformers la idhukku solution)  \n",
    "âŒ **Hardware Demanding** - High memory bandwidth venum\n",
    "\n",
    "\n",
    "\n",
    "## 9ï¸âƒ£ Key Formulas - Quick Reference\n",
    "\n",
    "### All Gates Together:\n",
    "\n",
    "\n",
    "1. Forget Gate:     f_t = Ïƒ(W_f Â· [h_{t-1}, x_t] + b_f)\n",
    "2. Input Gate:      i_t = Ïƒ(W_i Â· [h_{t-1}, x_t] + b_i)\n",
    "3. Candidate:       CÌƒ_t = tanh(W_C Â· [h_{t-1}, x_t] + b_C)\n",
    "4. Cell State:      C_t = f_t Â· C_{t-1} + i_t Â· CÌƒ_t\n",
    "5. Output Gate:     o_t = Ïƒ(W_o Â· [h_{t-1}, x_t] + b_o)\n",
    "6. Hidden State:    h_t = o_t Â· tanh(C_t)\n",
    "\n",
    "\n",
    "### Activation Functions:\n",
    "\n",
    "- **Sigmoid (Ïƒ):** 0 to 1 â†’ Gate control (0=close, 1=open)\n",
    "- **Tanh:** -1 to +1 â†’ Value creation/transformation\n",
    "\n",
    "\n",
    "\n",
    "## ðŸ”Ÿ Key Takeaways - Summary Points\n",
    "\n",
    "ðŸŽ¯ **LSTM = RNN + Gates** (Long-term memory solution)  \n",
    "ðŸŽ¯ **3 Gates:** Forget, Input, Output  \n",
    "ðŸŽ¯ **2 States:** Cell State (long-term) + Hidden State (short-term)  \n",
    "ðŸŽ¯ **Solves:** Vanishing gradient problem  \n",
    "ðŸŽ¯ **Trade-off:** Better performance but slower training  \n",
    "ðŸŽ¯ **Alternatives:** GRU (faster), Transformers (modern)  \n",
    "ðŸŽ¯ **Best for:** Sequential data with long-term dependencies\n",
    "\n",
    "\n",
    "\n",
    "# ðŸ’» PyTorch Implementation Examples\n",
    "\n",
    "## Example 1: Basic LSTM Layer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# LSTM Parameters\n",
    "input_size = 10      # Input features (e.g., word embedding dimension)\n",
    "hidden_size = 20     # LSTM cell oda memory capacity\n",
    "num_layers = 2       # Stacked LSTM layers\n",
    "batch_size = 5       # Number of sequences\n",
    "seq_length = 7       # Sequence length (e.g., 7 words)\n",
    "\n",
    "# Create LSTM Layer\n",
    "lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "# Random Input Data (Batch, Sequence, Features)\n",
    "inputs = torch.randn(batch_size, seq_length, input_size)\n",
    "\n",
    "# Initial States (Hidden State + Cell State)\n",
    "h0 = torch.zeros(num_layers, batch_size, hidden_size)\n",
    "c0 = torch.zeros(num_layers, batch_size, hidden_size)\n",
    "\n",
    "# Forward Pass\n",
    "output, (hn, cn) = lstm(inputs, (h0, c0))\n",
    "\n",
    "print(\"Input Shape:\", inputs.shape)          # [5, 7, 10]\n",
    "print(\"Output Shape:\", output.shape)         # [5, 7, 20]\n",
    "print(\"Final Hidden State:\", hn.shape)       # [2, 5, 20]\n",
    "print(\"Final Cell State:\", cn.shape)         # [2, 5, 20]\n",
    "\n",
    "\"\"\"\n",
    "Explanation:\n",
    "- output: Each time step oda hidden state (ovvoru word kum)\n",
    "- hn: Last time step oda hidden state mattum\n",
    "- cn: Last time step oda cell state (long-term memory)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "## Example 2: Sentiment Analysis Model (Complete)\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class SentimentLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout=0.5):\n",
    "        super(SentimentLSTM, self).__init__()\n",
    "\n",
    "        # Embedding Layer (Words ah vectors ah convert pannum)\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # LSTM Layer\n",
    "        self.lstm = nn.LSTM(embedding_dim,\n",
    "                           hidden_dim,\n",
    "                           num_layers=n_layers,\n",
    "                           batch_first=True,\n",
    "                           dropout=dropout if n_layers > 1 else 0)\n",
    "\n",
    "        # Fully Connected Layer (Classification)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text):\n",
    "        # text shape: [batch_size, seq_length]\n",
    "\n",
    "        # Embedding\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        # embedded shape: [batch_size, seq_length, embedding_dim]\n",
    "\n",
    "        # LSTM\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        # output shape: [batch_size, seq_length, hidden_dim]\n",
    "        # hidden shape: [n_layers, batch_size, hidden_dim]\n",
    "\n",
    "        # Last layer oda last hidden state ah edukurom\n",
    "        hidden = self.dropout(hidden[-1])  # [batch_size, hidden_dim]\n",
    "\n",
    "        # Classification\n",
    "        out = self.fc(hidden)  # [batch_size, output_dim]\n",
    "\n",
    "        return out\n",
    "\n",
    "# Model Parameters\n",
    "VOCAB_SIZE = 10000      # Total unique words\n",
    "EMBEDDING_DIM = 100     # Word vector size\n",
    "HIDDEN_DIM = 256        # LSTM memory size\n",
    "OUTPUT_DIM = 2          # Binary classification (Positive/Negative)\n",
    "N_LAYERS = 2            # Stacked LSTM\n",
    "DROPOUT = 0.5\n",
    "\n",
    "# Create Model\n",
    "model = SentimentLSTM(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, DROPOUT)\n",
    "\n",
    "print(model)\n",
    "print(f\"\\nTotal Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "\"\"\"\n",
    "Model Architecture:\n",
    "1. Embedding: Words â†’ Vectors\n",
    "2. LSTM: Sequential processing\n",
    "3. Dropout: Prevent overfitting\n",
    "4. FC: Final classification\n",
    "\n",
    "Example Usage:\n",
    "- Input: \"This movie is amazing!\" â†’ [45, 234, 12, 789, 23]\n",
    "- Embedding: Each word ID â†’ 100-dim vector\n",
    "- LSTM: Process sequence â†’ 256-dim hidden state\n",
    "- FC: 256 â†’ 2 (Positive/Negative scores)\n",
    "- Softmax: Probabilities\n",
    "\"\"\"\n",
    "\n",
    "## Example 3: Text Generation (Character-Level LSTM)\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class CharLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers):\n",
    "        super(CharLSTM, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # Character Embedding\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # LSTM\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, batch_first=True)\n",
    "\n",
    "        # Output Layer\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        # x shape: [batch_size, seq_length]\n",
    "\n",
    "        # Embedding\n",
    "        embed = self.embedding(x)\n",
    "        # embed shape: [batch_size, seq_length, embedding_dim]\n",
    "\n",
    "        # LSTM\n",
    "        out, hidden = self.lstm(embed, hidden)\n",
    "        # out shape: [batch_size, seq_length, hidden_dim]\n",
    "\n",
    "        # Reshape for FC layer\n",
    "        out = out.reshape(-1, self.hidden_dim)\n",
    "\n",
    "        # FC\n",
    "        out = self.fc(out)\n",
    "        # out shape: [batch_size * seq_length, vocab_size]\n",
    "\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size, device):\n",
    "        # Initialize hidden and cell states\n",
    "        h0 = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n",
    "        c0 = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n",
    "        return (h0, c0)\n",
    "\n",
    "# Example: Text Generation Function\n",
    "def generate_text(model, start_text, char_to_idx, idx_to_char, length=100, temperature=1.0):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    # Convert start text to indices\n",
    "    chars = [char_to_idx[c] for c in start_text]\n",
    "    input_seq = torch.tensor(chars).unsqueeze(0).to(device)  # [1, len(start_text)]\n",
    "\n",
    "    # Initialize hidden state\n",
    "    hidden = model.init_hidden(1, device)\n",
    "\n",
    "    generated = start_text\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(length):\n",
    "            # Forward pass\n",
    "            out, hidden = model(input_seq, hidden)\n",
    "\n",
    "            # Get last prediction\n",
    "            logits = out[-1] / temperature\n",
    "            probs = torch.softmax(logits, dim=0)\n",
    "\n",
    "            # Sample next character\n",
    "            next_idx = torch.multinomial(probs, 1).item()\n",
    "            next_char = idx_to_char[next_idx]\n",
    "\n",
    "            generated += next_char\n",
    "\n",
    "            # Update input\n",
    "            input_seq = torch.tensor([[next_idx]]).to(device)\n",
    "\n",
    "    return generated\n",
    "\n",
    "\"\"\"\n",
    "Example Usage:\n",
    "- Input: \"Hello\"\n",
    "- Model predicts next character based on previous characters\n",
    "- Output: \"Hello world, this is an LSTM generated text...\"\n",
    "\n",
    "Temperature parameter:\n",
    "- Low (0.5): More predictable, conservative\n",
    "- High (1.5): More random, creative\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Example 4: Bidirectional LSTM\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(BiLSTM, self).__init__()\n",
    "\n",
    "        # Bidirectional LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True  # Key parameter!\n",
    "        )\n",
    "\n",
    "        # FC layer (hidden_size * 2 because bidirectional)\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, seq_length, input_size]\n",
    "\n",
    "        # LSTM\n",
    "        lstm_out, (hidden, cell) = self.lstm(x)\n",
    "        # lstm_out shape: [batch_size, seq_length, hidden_size * 2]\n",
    "\n",
    "        # Use last time step's output\n",
    "        # lstm_out[:, -1, :] takes the last time step for all batches\n",
    "        out = self.fc(lstm_out[:, -1, :])\n",
    "\n",
    "        return out\n",
    "\n",
    "# Create Bidirectional LSTM\n",
    "bi_lstm = BiLSTM(input_size=50, hidden_size=128, output_size=3, num_layers=2)\n",
    "\n",
    "# Test\n",
    "test_input = torch.randn(32, 10, 50)  # [batch=32, seq=10, features=50]\n",
    "output = bi_lstm(test_input)\n",
    "print(\"Output shape:\", output.shape)  # [32, 3]\n",
    "\n",
    "\"\"\"\n",
    "Bidirectional LSTM Advantages:\n",
    "- Forward direction: Past context\n",
    "- Backward direction: Future context\n",
    "- Both combine â†’ Better understanding\n",
    "\n",
    "Example:\n",
    "Sentence: \"The cat sat on the mat\"\n",
    "- Forward LSTM: \"The\" â†’ \"cat\" â†’ \"sat\" (past context)\n",
    "- Backward LSTM: \"mat\" â†’ \"the\" â†’ \"on\" (future context)\n",
    "- Combined: Full sentence context for each word\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Example 5: Many-to-Many LSTM (Sequence to Sequence)\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Seq2SeqLSTM(nn.Module):\n",
    "    def __init__(self, input_vocab_size, output_vocab_size, embedding_dim, hidden_dim):\n",
    "        super(Seq2SeqLSTM, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder_embedding = nn.Embedding(input_vocab_size, embedding_dim)\n",
    "        self.encoder = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder_embedding = nn.Embedding(output_vocab_size, embedding_dim)\n",
    "        self.decoder = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "        # Output\n",
    "        self.fc = nn.Linear(hidden_dim, output_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        # src: Source sequence (e.g., English)\n",
    "        # tgt: Target sequence (e.g., Tamil)\n",
    "\n",
    "        # Encode\n",
    "        src_embed = self.encoder_embedding(src)\n",
    "        _, (hidden, cell) = self.encoder(src_embed)\n",
    "\n",
    "        # Decode\n",
    "        tgt_embed = self.decoder_embedding(tgt)\n",
    "        decoder_out, _ = self.decoder(tgt_embed, (hidden, cell))\n",
    "\n",
    "        # Output\n",
    "        output = self.fc(decoder_out)\n",
    "\n",
    "        return output\n",
    "\n",
    "\"\"\"\n",
    "Seq2Seq Application: Language Translation\n",
    "Input: \"I love programming\" (English)\n",
    "Process:\n",
    "1. Encoder: English sentence ah encode pannum â†’ context vector\n",
    "2. Decoder: Context vector use panni Tamil la decode pannum\n",
    "Output: \"Naan programming ah love pannuren\" (Tamil)\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ“ Practice Exercise\n",
    "\n",
    "\n",
    "# Exercise: Create a simple LSTM for sequence classification\n",
    "\n",
    "# TODO 1: Define LSTM model with following specs:\n",
    "#   - Vocab size: 5000\n",
    "#   - Embedding dimension: 100\n",
    "#   - Hidden dimension: 128\n",
    "#   - Output classes: 3 (Multi-class classification)\n",
    "#   - Layers: 2\n",
    "\n",
    "# TODO 2: Create sample input data\n",
    "#   - Batch size: 16\n",
    "#   - Sequence length: 20\n",
    "\n",
    "# TODO 3: Forward pass and print shapes\n",
    "\n",
    "# TODO 4: Calculate total parameters in model\n",
    "\n",
    "# Solution below (try first before looking!)\n",
    "```\n",
    "\n",
    "<details>\n",
    "<summary>Solution (Click to expand)</summary>\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Solution\n",
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim, n_layers):\n",
    "        super(SimpleLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embed = self.embedding(x)\n",
    "        _, (hidden, _) = self.lstm(embed)\n",
    "        out = self.fc(hidden[-1])\n",
    "        return out\n",
    "\n",
    "# Create model\n",
    "model = SimpleLSTM(5000, 100, 128, 3, 2)\n",
    "\n",
    "# Sample input\n",
    "batch_size, seq_len = 16, 20\n",
    "x = torch.randint(0, 5000, (batch_size, seq_len))\n",
    "\n",
    "# Forward pass\n",
    "output = model(x)\n",
    "print(\"Output shape:\", output.shape)  # [16, 3]\n",
    "\n",
    "# Total parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“š LSTM Complete Summary \n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 1ï¸âƒ£ LSTM Enna? Yean Venum? (What is LSTM? Why Do We Need It?)\n",
    "\n",
    "### RNN oda Problems:\n",
    "\n",
    "âŒ **Vanishing Gradient Problem** - Long sequences la pazhaya info ah remember pana mudiyathu  \n",
    "âŒ **No Control** - Edha remember pananum, edha forget pananum nu control illa  \n",
    "âŒ **Short-term Memory** - Arambathula padichadha marandhudum\n",
    "\n",
    "### LSTM - The Solution! ðŸ’¡\n",
    "\n",
    "- **LSTM = Long Short-Term Memory**\n",
    "- RNN oda advanced version\n",
    "- **Gates** mechanism use panni, edha remember pananum, edha forget pananum nu decide pannum\n",
    "- Long-term dependencies ah handle pana best!\n",
    "\n",
    "**Real-Life Example:**\n",
    "\n",
    "```\n",
    "\"I grew up in France... (100 words later)... I speak fluent ____\"\n",
    "```\n",
    "\n",
    "- RNN: \"France\" ah marandhidum â†’ Thappu answer\n",
    "- LSTM: \"France\" ah remember vechukum â†’ \"French\" nu correct ah predict pannum\n",
    "\n",
    "---\n",
    "\n",
    "## 2ï¸âƒ£ LSTM Architecture - The 3 Gates System ðŸšª\n",
    "\n",
    "LSTM la **4 main components** irukum:\n",
    "\n",
    "1. **Cell State (C_t)** - Long-term memory storage (Main highway)\n",
    "2. **Hidden State (h_t)** - Short-term memory (Current output)\n",
    "3. **Three Gates** - Information flow control pannum\n",
    "\n",
    "### Gate 1: Forget Gate (f_t) ðŸ—‘ï¸\n",
    "\n",
    "**Purpose:** Pazhaya information la edha delete pananum nu decide pannum\n",
    "\n",
    "**Formula:**\n",
    "$$f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)$$\n",
    "\n",
    "**Example:**\n",
    "\n",
    "- Input: \"The cat was black. **The dog** was white.\"\n",
    "- Forget Gate: \"cat\" pathi info ah forget pannidum (because new subject \"dog\" vandhuduchu)\n",
    "\n",
    "**How it works:**\n",
    "\n",
    "- Sigmoid function use pannum (output: 0 to 1)\n",
    "- 0 = Completely forget\n",
    "- 1 = Completely remember\n",
    "\n",
    "---\n",
    "\n",
    "### Gate 2: Input Gate (i_t) ðŸ“¥\n",
    "\n",
    "**Purpose:** Puthiya information la edha add pananum nu decide pannum\n",
    "\n",
    "**Formulas:**\n",
    "$$i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)$$\n",
    "$$\\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)$$\n",
    "\n",
    "**Example:**\n",
    "\n",
    "- New word: \"beautiful\"\n",
    "- Input Gate: Idhukku important nu mudivu panni cell state ku add pannum\n",
    "\n",
    "**Two Steps:**\n",
    "\n",
    "1. **Decide** what to add (sigmoid function)\n",
    "2. **Create** candidate values (tanh function: -1 to +1)\n",
    "\n",
    "---\n",
    "\n",
    "### Gate 3: Output Gate (o_t) ðŸ“¤\n",
    "\n",
    "**Purpose:** Current step la enna output kudukanum nu decide pannum\n",
    "\n",
    "**Formulas:**\n",
    "$$o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)$$\n",
    "$$h_t = o_t \\cdot \\tanh(C_t)$$\n",
    "\n",
    "**Example:**\n",
    "\n",
    "- Sentence half complete: \"The weather is\"\n",
    "- Output Gate: Current context based ah \"sunny\" or \"rainy\" nu predict pannum\n",
    "\n",
    "---\n",
    "\n",
    "### Cell State Update ðŸ”„\n",
    "\n",
    "**Formula:**\n",
    "$$C_t = f_t \\cdot C_{t-1} + i_t \\cdot \\tilde{C}_t$$\n",
    "\n",
    "**Breakdown:**\n",
    "\n",
    "- `f_t Â· C_{t-1}` â†’ Pazhaya memory la edha keep pananum\n",
    "- `i_t Â· CÌƒ_t` â†’ Puthiya info la edha add pananum\n",
    "- Both ah combine panni new cell state create pannum\n",
    "\n",
    "---\n",
    "\n",
    "## 3ï¸âƒ£ LSTM vs RNN - Quick Comparison\n",
    "\n",
    "| **Feature**            | **RNN**         | **LSTM**                        |\n",
    "| ---------------------- | --------------- | ------------------------------- |\n",
    "| **Memory**             | Short-term only | Long-term + Short-term          |\n",
    "| **Gates**              | No gates        | 3 Gates (Forget, Input, Output) |\n",
    "| **Vanishing Gradient** | Problem irukum  | Almost solved                   |\n",
    "| **Complexity**         | Simple          | Complex (more parameters)       |\n",
    "| **Training Time**      | Fast            | Slower                          |\n",
    "| **Use Cases**          | Short sequences | Long sequences                  |\n",
    "\n",
    "---\n",
    "\n",
    "## 4ï¸âƒ£ LSTM Variants (Different Types)\n",
    "\n",
    "### a) Vanilla LSTM (Standard LSTM)\n",
    "\n",
    "- 3 gates + cell state\n",
    "- Most commonly used version\n",
    "\n",
    "### b) Peephole LSTM\n",
    "\n",
    "- Gates ku direct ah cell state access kudukurom\n",
    "- More control over information flow\n",
    "\n",
    "### c) Deep LSTM (Stacked LSTM)\n",
    "\n",
    "- Multiple LSTM layers one after another\n",
    "- Complex patterns learn pana use aagum\n",
    "\n",
    "### d) Bidirectional LSTM\n",
    "\n",
    "- Forward + Backward directions la process pannum\n",
    "- Full context (past + future) therijrum\n",
    "- **Use case:** Named Entity Recognition, Speech Recognition\n",
    "\n",
    "### e) GRU (Gated Recurrent Unit)\n",
    "\n",
    "- Simplified LSTM version\n",
    "- Only 2 gates (Update gate + Reset gate)\n",
    "- Faster training, LSTM ku similar performance\n",
    "\n",
    "---\n",
    "\n",
    "## 5ï¸âƒ£ GRU vs LSTM - Detailed Comparison\n",
    "\n",
    "| **Aspect**      | **LSTM**                             | **GRU**                                   |\n",
    "| --------------- | ------------------------------------ | ----------------------------------------- |\n",
    "| **Gates**       | 3 Gates                              | 2 Gates                                   |\n",
    "| **Cell State**  | Separate cell state irukum           | No separate cell state                    |\n",
    "| **Parameters**  | More parameters                      | Less parameters                           |\n",
    "| **Training**    | Slower                               | Faster                                    |\n",
    "| **Memory**      | More memory needed                   | Less memory                               |\n",
    "| **Performance** | Slightly better for complex tasks    | Similar, sometimes better                 |\n",
    "| **Best For**    | Long sequences with complex patterns | Simpler sequences, faster training needed |\n",
    "\n",
    "**When to use what?**\n",
    "\n",
    "- **LSTM:** Long sequences, complex patterns, more data available\n",
    "- **GRU:** Faster training needed, less data, simpler patterns\n",
    "\n",
    "---\n",
    "\n",
    "## 6ï¸âƒ£ LSTM Applications - Real World Examples\n",
    "\n",
    "### ðŸ“ Natural Language Processing (NLP)\n",
    "\n",
    "- **Language Translation:** English â†’ Tamil conversion\n",
    "- **Text Generation:** Next word/sentence prediction\n",
    "- **Sentiment Analysis:** Movie reviews ah positive/negative nu classify\n",
    "- **Chatbots:** Conversational AI\n",
    "\n",
    "### ðŸŽ¤ Speech Recognition\n",
    "\n",
    "- **Voice Assistants:** Siri, Alexa, Google Assistant\n",
    "- **Speech-to-Text:** Audio ah text ah convert pannum\n",
    "- **Voice Commands:** \"Hey Google, turn on lights\"\n",
    "\n",
    "### ðŸ–¼ï¸ Image Processing\n",
    "\n",
    "- **Image Captioning:** Image paathu description generate pannum\n",
    "  - Example: \"A dog playing in the park\"\n",
    "- **Video Analysis:** Frame-by-frame sequence analysis\n",
    "\n",
    "### ðŸŽµ Music Generation\n",
    "\n",
    "- Notes sequence based ah new music create pannum\n",
    "- Classical music patterns learn panni generate pannum\n",
    "\n",
    "### ðŸ“ˆ Time Series Prediction\n",
    "\n",
    "- **Stock Market:** Price predictions\n",
    "- **Weather Forecast:** Temperature, rainfall predictions\n",
    "- **Sales Forecasting:** Future sales predict pannum\n",
    "\n",
    "### âœï¸ Handwriting Recognition\n",
    "\n",
    "- Handwritten text ah digital text ah convert pannum\n",
    "- Signature verification\n",
    "\n",
    "---\n",
    "\n",
    "## 7ï¸âƒ£ LSTM Advantages âœ…\n",
    "\n",
    "ðŸŽ¯ **Long-term Memory** - Vanishing gradient problem ah solve pannum  \n",
    "ðŸŽ¯ **Selective Memory** - Gates use panni important info mattum remember pannum  \n",
    "ðŸŽ¯ **Flexible** - Variable length sequences handle panlam  \n",
    "ðŸŽ¯ **Versatile** - Text, audio, video, time-series data ellathukum use panlam  \n",
    "ðŸŽ¯ **Better Context** - Past information ah effectively use pannum\n",
    "\n",
    "---\n",
    "\n",
    "## 8ï¸âƒ£ LSTM Limitations âŒ\n",
    "\n",
    "âŒ **Training Time** - RNN vida slow ah train aagum  \n",
    "âŒ **Computational Cost** - More parameters â†’ More memory + processing power venum  \n",
    "âŒ **Still Gradient Issues** - Completely vanishing gradient solve aagala  \n",
    "âŒ **Overfitting** - Small datasets la overfit aaga chances irukum  \n",
    "âŒ **Sequential Processing** - Parallel processing pana mudiyathu (Transformers la idhukku solution)  \n",
    "âŒ **Hardware Demanding** - High memory bandwidth venum\n",
    "\n",
    "---\n",
    "\n",
    "## 9ï¸âƒ£ Key Formulas - Quick Reference\n",
    "\n",
    "### All Gates Together:\n",
    "\n",
    "```\n",
    "1. Forget Gate:     f_t = Ïƒ(W_f Â· [h_{t-1}, x_t] + b_f)\n",
    "2. Input Gate:      i_t = Ïƒ(W_i Â· [h_{t-1}, x_t] + b_i)\n",
    "3. Candidate:       CÌƒ_t = tanh(W_C Â· [h_{t-1}, x_t] + b_C)\n",
    "4. Cell State:      C_t = f_t Â· C_{t-1} + i_t Â· CÌƒ_t\n",
    "5. Output Gate:     o_t = Ïƒ(W_o Â· [h_{t-1}, x_t] + b_o)\n",
    "6. Hidden State:    h_t = o_t Â· tanh(C_t)\n",
    "```\n",
    "\n",
    "### Activation Functions:\n",
    "\n",
    "- **Sigmoid (Ïƒ):** 0 to 1 â†’ Gate control (0=close, 1=open)\n",
    "- **Tanh:** -1 to +1 â†’ Value creation/transformation\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”Ÿ Key Takeaways - Summary Points\n",
    "\n",
    "ðŸŽ¯ **LSTM = RNN + Gates** (Long-term memory solution)  \n",
    "ðŸŽ¯ **3 Gates:** Forget, Input, Output  \n",
    "ðŸŽ¯ **2 States:** Cell State (long-term) + Hidden State (short-term)  \n",
    "ðŸŽ¯ **Solves:** Vanishing gradient problem  \n",
    "ðŸŽ¯ **Trade-off:** Better performance but slower training  \n",
    "ðŸŽ¯ **Alternatives:** GRU (faster), Transformers (modern)  \n",
    "ðŸŽ¯ **Best for:** Sequential data with long-term dependencies\n",
    "\n",
    "---\n",
    "\n",
    "# ðŸ’» PyTorch Implementation Examples\n",
    "\n",
    "## Example 1: Basic LSTM Layer\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# LSTM Parameters\n",
    "input_size = 10      # Input features (e.g., word embedding dimension)\n",
    "hidden_size = 20     # LSTM cell oda memory capacity\n",
    "num_layers = 2       # Stacked LSTM layers\n",
    "batch_size = 5       # Number of sequences\n",
    "seq_length = 7       # Sequence length (e.g., 7 words)\n",
    "\n",
    "# Create LSTM Layer\n",
    "lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "# Random Input Data (Batch, Sequence, Features)\n",
    "inputs = torch.randn(batch_size, seq_length, input_size)\n",
    "\n",
    "# Initial States (Hidden State + Cell State)\n",
    "h0 = torch.zeros(num_layers, batch_size, hidden_size)\n",
    "c0 = torch.zeros(num_layers, batch_size, hidden_size)\n",
    "\n",
    "# Forward Pass\n",
    "output, (hn, cn) = lstm(inputs, (h0, c0))\n",
    "\n",
    "print(\"Input Shape:\", inputs.shape)          # [5, 7, 10]\n",
    "print(\"Output Shape:\", output.shape)         # [5, 7, 20]\n",
    "print(\"Final Hidden State:\", hn.shape)       # [2, 5, 20]\n",
    "print(\"Final Cell State:\", cn.shape)         # [2, 5, 20]\n",
    "\n",
    "\"\"\"\n",
    "Explanation:\n",
    "- output: Each time step oda hidden state (ovvoru word kum)\n",
    "- hn: Last time step oda hidden state mattum\n",
    "- cn: Last time step oda cell state (long-term memory)\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Example 2: Sentiment Analysis Model (Complete)\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class SentimentLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout=0.5):\n",
    "        super(SentimentLSTM, self).__init__()\n",
    "\n",
    "        # Embedding Layer (Words ah vectors ah convert pannum)\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # LSTM Layer\n",
    "        self.lstm = nn.LSTM(embedding_dim,\n",
    "                           hidden_dim,\n",
    "                           num_layers=n_layers,\n",
    "                           batch_first=True,\n",
    "                           dropout=dropout if n_layers > 1 else 0)\n",
    "\n",
    "        # Fully Connected Layer (Classification)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text):\n",
    "        # text shape: [batch_size, seq_length]\n",
    "\n",
    "        # Embedding\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        # embedded shape: [batch_size, seq_length, embedding_dim]\n",
    "\n",
    "        # LSTM\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        # output shape: [batch_size, seq_length, hidden_dim]\n",
    "        # hidden shape: [n_layers, batch_size, hidden_dim]\n",
    "\n",
    "        # Last layer oda last hidden state ah edukurom\n",
    "        hidden = self.dropout(hidden[-1])  # [batch_size, hidden_dim]\n",
    "\n",
    "        # Classification\n",
    "        out = self.fc(hidden)  # [batch_size, output_dim]\n",
    "\n",
    "        return out\n",
    "\n",
    "# Model Parameters\n",
    "VOCAB_SIZE = 10000      # Total unique words\n",
    "EMBEDDING_DIM = 100     # Word vector size\n",
    "HIDDEN_DIM = 256        # LSTM memory size\n",
    "OUTPUT_DIM = 2          # Binary classification (Positive/Negative)\n",
    "N_LAYERS = 2            # Stacked LSTM\n",
    "DROPOUT = 0.5\n",
    "\n",
    "# Create Model\n",
    "model = SentimentLSTM(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, DROPOUT)\n",
    "\n",
    "print(model)\n",
    "print(f\"\\nTotal Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "\"\"\"\n",
    "Model Architecture:\n",
    "1. Embedding: Words â†’ Vectors\n",
    "2. LSTM: Sequential processing\n",
    "3. Dropout: Prevent overfitting\n",
    "4. FC: Final classification\n",
    "\n",
    "Example Usage:\n",
    "- Input: \"This movie is amazing!\" â†’ [45, 234, 12, 789, 23]\n",
    "- Embedding: Each word ID â†’ 100-dim vector\n",
    "- LSTM: Process sequence â†’ 256-dim hidden state\n",
    "- FC: 256 â†’ 2 (Positive/Negative scores)\n",
    "- Softmax: Probabilities\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Example 3: Text Generation (Character-Level LSTM)\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class CharLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers):\n",
    "        super(CharLSTM, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # Character Embedding\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # LSTM\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, batch_first=True)\n",
    "\n",
    "        # Output Layer\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        # x shape: [batch_size, seq_length]\n",
    "\n",
    "        # Embedding\n",
    "        embed = self.embedding(x)\n",
    "        # embed shape: [batch_size, seq_length, embedding_dim]\n",
    "\n",
    "        # LSTM\n",
    "        out, hidden = self.lstm(embed, hidden)\n",
    "        # out shape: [batch_size, seq_length, hidden_dim]\n",
    "\n",
    "        # Reshape for FC layer\n",
    "        out = out.reshape(-1, self.hidden_dim)\n",
    "\n",
    "        # FC\n",
    "        out = self.fc(out)\n",
    "        # out shape: [batch_size * seq_length, vocab_size]\n",
    "\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size, device):\n",
    "        # Initialize hidden and cell states\n",
    "        h0 = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n",
    "        c0 = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n",
    "        return (h0, c0)\n",
    "\n",
    "# Example: Text Generation Function\n",
    "def generate_text(model, start_text, char_to_idx, idx_to_char, length=100, temperature=1.0):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    # Convert start text to indices\n",
    "    chars = [char_to_idx[c] for c in start_text]\n",
    "    input_seq = torch.tensor(chars).unsqueeze(0).to(device)  # [1, len(start_text)]\n",
    "\n",
    "    # Initialize hidden state\n",
    "    hidden = model.init_hidden(1, device)\n",
    "\n",
    "    generated = start_text\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(length):\n",
    "            # Forward pass\n",
    "            out, hidden = model(input_seq, hidden)\n",
    "\n",
    "            # Get last prediction\n",
    "            logits = out[-1] / temperature\n",
    "            probs = torch.softmax(logits, dim=0)\n",
    "\n",
    "            # Sample next character\n",
    "            next_idx = torch.multinomial(probs, 1).item()\n",
    "            next_char = idx_to_char[next_idx]\n",
    "\n",
    "            generated += next_char\n",
    "\n",
    "            # Update input\n",
    "            input_seq = torch.tensor([[next_idx]]).to(device)\n",
    "\n",
    "    return generated\n",
    "\n",
    "\"\"\"\n",
    "Example Usage:\n",
    "- Input: \"Hello\"\n",
    "- Model predicts next character based on previous characters\n",
    "- Output: \"Hello world, this is an LSTM generated text...\"\n",
    "\n",
    "Temperature parameter:\n",
    "- Low (0.5): More predictable, conservative\n",
    "- High (1.5): More random, creative\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Example 4: Bidirectional LSTM\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(BiLSTM, self).__init__()\n",
    "\n",
    "        # Bidirectional LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True  # Key parameter!\n",
    "        )\n",
    "\n",
    "        # FC layer (hidden_size * 2 because bidirectional)\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, seq_length, input_size]\n",
    "\n",
    "        # LSTM\n",
    "        lstm_out, (hidden, cell) = self.lstm(x)\n",
    "        # lstm_out shape: [batch_size, seq_length, hidden_size * 2]\n",
    "\n",
    "        # Use last time step's output\n",
    "        # lstm_out[:, -1, :] takes the last time step for all batches\n",
    "        out = self.fc(lstm_out[:, -1, :])\n",
    "\n",
    "        return out\n",
    "\n",
    "# Create Bidirectional LSTM\n",
    "bi_lstm = BiLSTM(input_size=50, hidden_size=128, output_size=3, num_layers=2)\n",
    "\n",
    "# Test\n",
    "test_input = torch.randn(32, 10, 50)  # [batch=32, seq=10, features=50]\n",
    "output = bi_lstm(test_input)\n",
    "print(\"Output shape:\", output.shape)  # [32, 3]\n",
    "\n",
    "\"\"\"\n",
    "Bidirectional LSTM Advantages:\n",
    "- Forward direction: Past context\n",
    "- Backward direction: Future context\n",
    "- Both combine â†’ Better understanding\n",
    "\n",
    "Example:\n",
    "Sentence: \"The cat sat on the mat\"\n",
    "- Forward LSTM: \"The\" â†’ \"cat\" â†’ \"sat\" (past context)\n",
    "- Backward LSTM: \"mat\" â†’ \"the\" â†’ \"on\" (future context)\n",
    "- Combined: Full sentence context for each word\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Example 5: Many-to-Many LSTM (Sequence to Sequence)\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Seq2SeqLSTM(nn.Module):\n",
    "    def __init__(self, input_vocab_size, output_vocab_size, embedding_dim, hidden_dim):\n",
    "        super(Seq2SeqLSTM, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder_embedding = nn.Embedding(input_vocab_size, embedding_dim)\n",
    "        self.encoder = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder_embedding = nn.Embedding(output_vocab_size, embedding_dim)\n",
    "        self.decoder = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "        # Output\n",
    "        self.fc = nn.Linear(hidden_dim, output_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        # src: Source sequence (e.g., English)\n",
    "        # tgt: Target sequence (e.g., Tamil)\n",
    "\n",
    "        # Encode\n",
    "        src_embed = self.encoder_embedding(src)\n",
    "        _, (hidden, cell) = self.encoder(src_embed)\n",
    "\n",
    "        # Decode\n",
    "        tgt_embed = self.decoder_embedding(tgt)\n",
    "        decoder_out, _ = self.decoder(tgt_embed, (hidden, cell))\n",
    "\n",
    "        # Output\n",
    "        output = self.fc(decoder_out)\n",
    "\n",
    "        return output\n",
    "\n",
    "\"\"\"\n",
    "Seq2Seq Application: Language Translation\n",
    "Input: \"I love programming\" (English)\n",
    "Process:\n",
    "1. Encoder: English sentence ah encode pannum â†’ context vector\n",
    "2. Decoder: Context vector use panni Tamil la decode pannum\n",
    "Output: \"Naan programming ah love pannuren\" (Tamil)\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ“ Practice Exercise\n",
    "\n",
    "```python\n",
    "# Exercise: Create a simple LSTM for sequence classification\n",
    "\n",
    "# TODO 1: Define LSTM model with following specs:\n",
    "#   - Vocab size: 5000\n",
    "#   - Embedding dimension: 100\n",
    "#   - Hidden dimension: 128\n",
    "#   - Output classes: 3 (Multi-class classification)\n",
    "#   - Layers: 2\n",
    "\n",
    "# TODO 2: Create sample input data\n",
    "#   - Batch size: 16\n",
    "#   - Sequence length: 20\n",
    "\n",
    "# TODO 3: Forward pass and print shapes\n",
    "\n",
    "# TODO 4: Calculate total parameters in model\n",
    "\n",
    "# Solution below (try first before looking!)\n",
    "```\n",
    "\n",
    "<details>\n",
    "<summary>Solution (Click to expand)</summary>\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Solution\n",
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim, n_layers):\n",
    "        super(SimpleLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embed = self.embedding(x)\n",
    "        _, (hidden, _) = self.lstm(embed)\n",
    "        out = self.fc(hidden[-1])\n",
    "        return out\n",
    "\n",
    "# Create model\n",
    "model = SimpleLSTM(5000, 100, 128, 3, 2)\n",
    "\n",
    "# Sample input\n",
    "batch_size, seq_len = 16, 20\n",
    "x = torch.randint(0, 5000, (batch_size, seq_len))\n",
    "\n",
    "# Forward pass\n",
    "output = model(x)\n",
    "print(\"Output shape:\", output.shape)  # [16, 3]\n",
    "\n",
    "# Total parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "guvi_env_algo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
